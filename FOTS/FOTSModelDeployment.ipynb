{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FOTSModelDeployment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1WhnVO29AVG"
      },
      "source": [
        "!pip -q install streamlit\r\n",
        "!pip -q install pyngrok"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9I3nOFGh03EB"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKDBIGmr9y2U",
        "outputId": "5bfd7b3a-9de8-4ac1-ae46-2305ef3c6e57"
      },
      "source": [
        "%%writefile app.py\r\n",
        "#https://github.com/Poseyy/StreamlitDemos/tree/master/Streamlit_Upload\r\n",
        "import streamlit as st\r\n",
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "import cv2\r\n",
        "import numpy as np\r\n",
        "import threading\r\n",
        "import shutil\r\n",
        "import pandas as pd\r\n",
        "import os\r\n",
        "import math\r\n",
        "import csv\r\n",
        "import cv2\r\n",
        "import time\r\n",
        "import shutil\r\n",
        "import numpy as np\r\n",
        "import scipy.optimize\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import matplotlib.patches as Patches\r\n",
        "from shapely.geometry import Polygon\r\n",
        "import random\r\n",
        "from PIL import Image\r\n",
        "PAGE_CONFIG = {\"page_title\":\"FOTS:Scene Text Parsing\",\"layout\":\"centered\"}\r\n",
        "st.set_page_config(**PAGE_CONFIG)\r\n",
        "\r\n",
        "########################## CODE FOR SOME FUNCTIONS NEEDED TO CONVERT SCORE & GEO MAPS TO BOXES ############################\r\n",
        "\r\n",
        "def sort_poly(p):\r\n",
        "  min_axis = np.argmin(np.sum(p, axis=1))\r\n",
        "  p = p[[min_axis, (min_axis+1)%4, (min_axis+2)%4, (min_axis+3)%4]]\r\n",
        "  if abs(p[0, 0] - p[1, 0]) > abs(p[0, 1] - p[1, 1]):\r\n",
        "    return p\r\n",
        "  else:\r\n",
        "    return p[[0, 3, 2, 1]]\r\n",
        "def intersection(g, p):\r\n",
        "    g = Polygon(g[:8].reshape((4, 2)))\r\n",
        "    p = Polygon(p[:8].reshape((4, 2)))\r\n",
        "    if not g.is_valid or not p.is_valid:\r\n",
        "        return 0\r\n",
        "    inter = Polygon(g).intersection(Polygon(p)).area\r\n",
        "    union = g.area + p.area - inter\r\n",
        "    if union == 0:\r\n",
        "        return 0\r\n",
        "    else:\r\n",
        "        return inter/union\r\n",
        "\r\n",
        "\r\n",
        "def weighted_merge(g, p):\r\n",
        "    g[:8] = (g[8] * g[:8] + p[8] * p[:8])/(g[8] + p[8])\r\n",
        "    g[8] = (g[8] + p[8])\r\n",
        "    return g\r\n",
        "\r\n",
        "\r\n",
        "def standard_nms(S, thres):\r\n",
        "    order = np.argsort(S[:, 8])[::-1]\r\n",
        "    keep = []\r\n",
        "    while order.size > 0:\r\n",
        "        i = order[0]\r\n",
        "        keep.append(i)\r\n",
        "        ovr = np.array([intersection(S[i], S[t]) for t in order[1:]])\r\n",
        "\r\n",
        "        inds = np.where(ovr <= thres)[0]\r\n",
        "        order = order[inds+1]\r\n",
        "\r\n",
        "    return S[keep]\r\n",
        "\r\n",
        "\r\n",
        "def nms_locality(polys, thres=0.3):\r\n",
        "    '''\r\n",
        "    :param polys: a N*9 numpy array. first 8 coordinates, then prob\r\n",
        "    :return: boxes after nms\r\n",
        "    '''\r\n",
        "    S = []\r\n",
        "    p = None\r\n",
        "    for g in polys:\r\n",
        "        if p is not None and intersection(g, p) > thres:\r\n",
        "            p = weighted_merge(g, p)\r\n",
        "        else:\r\n",
        "            if p is not None:\r\n",
        "                S.append(p)\r\n",
        "            p = g\r\n",
        "    if p is not None:\r\n",
        "        S.append(p)\r\n",
        "\r\n",
        "    if len(S) == 0:\r\n",
        "        return np.array([])\r\n",
        "    return standard_nms(np.array(S), thres)\r\n",
        "\r\n",
        "def restore_rectangle_rbox(origin, geometry):\r\n",
        "    ''' Resotre rectangle tbox'''\r\n",
        "    d = geometry[:, :4]\r\n",
        "    angle = geometry[:, 4]\r\n",
        "    # for angle > 0\r\n",
        "    origin_0 = origin[angle >= 0]\r\n",
        "    d_0 = d[angle >= 0]\r\n",
        "    angle_0 = angle[angle >= 0]\r\n",
        "    if origin_0.shape[0] > 0:\r\n",
        "        p = np.array([np.zeros(d_0.shape[0]), -d_0[:, 0] - d_0[:, 2],\r\n",
        "                      d_0[:, 1] + d_0[:, 3], -d_0[:, 0] - d_0[:, 2],\r\n",
        "                      d_0[:, 1] + d_0[:, 3], np.zeros(d_0.shape[0]),\r\n",
        "                      np.zeros(d_0.shape[0]), np.zeros(d_0.shape[0]),\r\n",
        "                      d_0[:, 3], -d_0[:, 2]])\r\n",
        "        p = p.transpose((1, 0)).reshape((-1, 5, 2))  # N*5*2\r\n",
        "\r\n",
        "        rotate_matrix_x = np.array([np.cos(angle_0), np.sin(angle_0)]).transpose((1, 0))\r\n",
        "        rotate_matrix_x = np.repeat(rotate_matrix_x, 5, axis=1).reshape(-1, 2, 5).transpose((0, 2, 1))  # N*5*2\r\n",
        "\r\n",
        "        rotate_matrix_y = np.array([-np.sin(angle_0), np.cos(angle_0)]).transpose((1, 0))\r\n",
        "        rotate_matrix_y = np.repeat(rotate_matrix_y, 5, axis=1).reshape(-1, 2, 5).transpose((0, 2, 1))\r\n",
        "\r\n",
        "        p_rotate_x = np.sum(rotate_matrix_x * p, axis=2)[:, :, np.newaxis]  # N*5*1\r\n",
        "        p_rotate_y = np.sum(rotate_matrix_y * p, axis=2)[:, :, np.newaxis]  # N*5*1\r\n",
        "\r\n",
        "        p_rotate = np.concatenate([p_rotate_x, p_rotate_y], axis=2)  # N*5*2\r\n",
        "\r\n",
        "        p3_in_origin = origin_0 - p_rotate[:, 4, :]\r\n",
        "        new_p0 = p_rotate[:, 0, :] + p3_in_origin  # N*2\r\n",
        "        new_p1 = p_rotate[:, 1, :] + p3_in_origin\r\n",
        "        new_p2 = p_rotate[:, 2, :] + p3_in_origin\r\n",
        "        new_p3 = p_rotate[:, 3, :] + p3_in_origin\r\n",
        "\r\n",
        "        new_p_0 = np.concatenate([new_p0[:, np.newaxis, :], new_p1[:, np.newaxis, :],\r\n",
        "                                  new_p2[:, np.newaxis, :], new_p3[:, np.newaxis, :]], axis=1)  # N*4*2\r\n",
        "    else:\r\n",
        "        new_p_0 = np.zeros((0, 4, 2))\r\n",
        "    # for angle < 0\r\n",
        "    origin_1 = origin[angle < 0]\r\n",
        "    d_1 = d[angle < 0]\r\n",
        "    angle_1 = angle[angle < 0]\r\n",
        "    if origin_1.shape[0] > 0:\r\n",
        "        p = np.array([-d_1[:, 1] - d_1[:, 3], -d_1[:, 0] - d_1[:, 2],\r\n",
        "                      np.zeros(d_1.shape[0]), -d_1[:, 0] - d_1[:, 2],\r\n",
        "                      np.zeros(d_1.shape[0]), np.zeros(d_1.shape[0]),\r\n",
        "                      -d_1[:, 1] - d_1[:, 3], np.zeros(d_1.shape[0]),\r\n",
        "                      -d_1[:, 1], -d_1[:, 2]])\r\n",
        "        p = p.transpose((1, 0)).reshape((-1, 5, 2))  # N*5*2\r\n",
        "\r\n",
        "        rotate_matrix_x = np.array([np.cos(-angle_1), -np.sin(-angle_1)]).transpose((1, 0))\r\n",
        "        rotate_matrix_x = np.repeat(rotate_matrix_x, 5, axis=1).reshape(-1, 2, 5).transpose((0, 2, 1))  # N*5*2\r\n",
        "\r\n",
        "        rotate_matrix_y = np.array([np.sin(-angle_1), np.cos(-angle_1)]).transpose((1, 0))\r\n",
        "        rotate_matrix_y = np.repeat(rotate_matrix_y, 5, axis=1).reshape(-1, 2, 5).transpose((0, 2, 1))\r\n",
        "\r\n",
        "        p_rotate_x = np.sum(rotate_matrix_x * p, axis=2)[:, :, np.newaxis]  # N*5*1\r\n",
        "        p_rotate_y = np.sum(rotate_matrix_y * p, axis=2)[:, :, np.newaxis]  # N*5*1\r\n",
        "\r\n",
        "        p_rotate = np.concatenate([p_rotate_x, p_rotate_y], axis=2)  # N*5*2\r\n",
        "\r\n",
        "        p3_in_origin = origin_1 - p_rotate[:, 4, :]\r\n",
        "        new_p0 = p_rotate[:, 0, :] + p3_in_origin  # N*2\r\n",
        "        new_p1 = p_rotate[:, 1, :] + p3_in_origin\r\n",
        "        new_p2 = p_rotate[:, 2, :] + p3_in_origin\r\n",
        "        new_p3 = p_rotate[:, 3, :] + p3_in_origin\r\n",
        "\r\n",
        "        new_p_1 = np.concatenate([new_p0[:, np.newaxis, :], new_p1[:, np.newaxis, :],\r\n",
        "                                  new_p2[:, np.newaxis, :], new_p3[:, np.newaxis, :]], axis=1)  # N*4*2\r\n",
        "    else:\r\n",
        "        new_p_1 = np.zeros((0, 4, 2))\r\n",
        "    return np.concatenate([new_p_0, new_p_1])\r\n",
        "\r\n",
        "\r\n",
        "def restore_rectangle(origin, geometry):\r\n",
        "    return restore_rectangle_rbox(origin, geometry)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def generate_roiRotatePara(box, angle, expand_w = 60):\r\n",
        "    '''Generate all ROI Parameterts'''\r\n",
        "    p0_rect, p1_rect, p2_rect, p3_rect = box\r\n",
        "    cxy = (p0_rect + p2_rect) / 2.\r\n",
        "    size = np.array([np.linalg.norm(p0_rect - p1_rect), np.linalg.norm(p0_rect - p3_rect)])\r\n",
        "    rrect = np.concatenate([cxy, size])\r\n",
        "\r\n",
        "    box=np.array(box)\r\n",
        "\r\n",
        "    points=np.array(box, dtype=np.int32)\r\n",
        "    xmin=np.min(points[:,0])\r\n",
        "    xmax=np.max(points[:,0])\r\n",
        "    ymin=np.min(points[:,1])\r\n",
        "    ymax=np.max(points[:,1])\r\n",
        "    bbox = np.array([xmin, ymin, xmax, ymax])\r\n",
        "    if np.any(bbox < -expand_w):\r\n",
        "        return None\r\n",
        "    \r\n",
        "    rrect[:2] -= bbox[:2]\r\n",
        "    rrect[:2] -= rrect[2:] / 2\r\n",
        "    rrect[2:] += rrect[:2]\r\n",
        "\r\n",
        "    bbox[2:] -= bbox[:2]\r\n",
        "\r\n",
        "    rrect[::2] = np.clip(rrect[::2], 0, bbox[2])\r\n",
        "    rrect[1::2] = np.clip(rrect[1::2], 0, bbox[3])\r\n",
        "    rrect[2:] -= rrect[:2]\r\n",
        "    \r\n",
        "    return bbox.astype(np.int32), rrect.astype(np.int32), - angle\r\n",
        "\r\n",
        "def restore_roiRotatePara(box):\r\n",
        "    rectange, rotate_angle = sort_rectangle(box)\r\n",
        "    return generate_roiRotatePara(rectange, rotate_angle)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def get_boxes(score_map,geo_map):\r\n",
        "  score_map_thresh=0.5\r\n",
        "  box_thresh=0.1 \r\n",
        "  nms_thres=0.2\r\n",
        "  if len(score_map.shape) == 4:\r\n",
        "    score_map = score_map[0, :, :, 0]\r\n",
        "    geo_map = geo_map[0, :, :, :]\r\n",
        "  # filter the score map\r\n",
        "  xy_text = np.argwhere(score_map > score_map_thresh)\r\n",
        "  # sort the text boxes via the y axis\r\n",
        "  xy_text = xy_text[np.argsort(xy_text[:, 0])]\r\n",
        "  # restore\r\n",
        "  text_box_restored = restore_rectangle(xy_text[:, ::-1], geo_map[xy_text[:, 0], xy_text[:, 1], :]) # N*4*2\r\n",
        "  boxes = np.zeros((text_box_restored.shape[0], 9), dtype=np.float32)\r\n",
        "  boxes[:, :8] = text_box_restored.reshape((-1, 8))\r\n",
        "  boxes[:, 8] = score_map[xy_text[:, 0], xy_text[:, 1]]\r\n",
        "  boxes = nms_locality(boxes.astype(np.float64), nms_thres)\r\n",
        "  # boxes = np.concatenate([boxes, _boxes], axis=0)\r\n",
        "\r\n",
        "  # here we filter some low score boxes by the average score map, this is different from the orginal paper\r\n",
        "  for i, box in enumerate(boxes):\r\n",
        "    mask = np.zeros_like(score_map, dtype=np.uint8)\r\n",
        "    cv2.fillPoly(mask, box[:8].reshape((-1, 4, 2)).astype(np.int32), 1)\r\n",
        "    boxes[i, 8] = cv2.mean(score_map, mask)[0]\r\n",
        "    if i==4:\r\n",
        "      break\r\n",
        "  if len(boxes)>0:\r\n",
        "    boxes = boxes[boxes[:, 8] > box_thresh]\r\n",
        "  boxes[:,:8:2] = np.clip(boxes[:,:8:2], 0, 512 - 1)\r\n",
        "  boxes[:,1:8:2] = np.clip(boxes[:,1:8:2], 0, 512 - 1)  \r\n",
        "  res = []\r\n",
        "  result = []\r\n",
        "  if len(boxes)>0:\r\n",
        "    for box in boxes:\r\n",
        "      box_ =  box[:8].reshape((4, 2))\r\n",
        "      if np.linalg.norm(box_[0] - box_[1]) < 8 or np.linalg.norm(box_[3]-box_[0]) < 8:\r\n",
        "        continue\r\n",
        "      result.append(box_)\r\n",
        "  res.append(np.array(result, np.float32))   \r\n",
        "\r\n",
        "  box_index = []\r\n",
        "  brotateParas = []\r\n",
        "  filter_bsharedFeatures = []\r\n",
        "  for i in range(len(res)):\r\n",
        "    rotateParas = []\r\n",
        "    rboxes=res[i]\r\n",
        "    for j, rbox in enumerate(rboxes):\r\n",
        "      para = restore_roiRotatePara(rbox)\r\n",
        "      if para and min(para[1][2:]) > 8:\r\n",
        "        rotateParas.append(para)\r\n",
        "        box_index.append((i, j))\r\n",
        "  return rotateParas      \r\n",
        "\r\n",
        "\r\n",
        "#Sorting a rectangle to get all point in clockwies manner\r\n",
        "# https://github.com/Pay20Y/FOTS_TF\r\n",
        "# https://github.com/yu20103983/FOTS\r\n",
        "# https://github.com/Masao-Taketani/FOTS_OCR\r\n",
        "def sort_rectangle(poly):\r\n",
        "    '''sort the four coordinates of the polygon, points in poly should be sorted clockwise'''\r\n",
        "    # First find the lowest point\r\n",
        "    p_lowest = np.argmax(poly[:, 1])\r\n",
        "    if np.count_nonzero(poly[:, 1] == poly[p_lowest, 1]) == 2:\r\n",
        "        # if the bottom line is parallel to x-axis, then p0 must be the upper-left corner\r\n",
        "        p0_index = np.argmin(np.sum(poly, axis=1))\r\n",
        "        p1_index = (p0_index + 1) % 4\r\n",
        "        p2_index = (p0_index + 2) % 4\r\n",
        "        p3_index = (p0_index + 3) % 4\r\n",
        "        return poly[[p0_index, p1_index, p2_index, p3_index]], 0.\r\n",
        "    else:\r\n",
        "        # find the point that sits right to the lowest point\r\n",
        "        p_lowest_right = (p_lowest - 1) % 4\r\n",
        "        p_lowest_left = (p_lowest + 1) % 4\r\n",
        "        angle = np.arctan(-(poly[p_lowest][1] - poly[p_lowest_right][1])/(poly[p_lowest][0] - poly[p_lowest_right][0]))\r\n",
        "        # assert angle > 0\r\n",
        "        if angle <= 0:\r\n",
        "            print(angle, poly[p_lowest], poly[p_lowest_right])\r\n",
        "        if angle/np.pi * 180 > 45:\r\n",
        "            #this point is p2\r\n",
        "            p2_index = p_lowest\r\n",
        "            p1_index = (p2_index - 1) % 4\r\n",
        "            p0_index = (p2_index - 2) % 4\r\n",
        "            p3_index = (p2_index + 1) % 4\r\n",
        "            return poly[[p0_index, p1_index, p2_index, p3_index]], -(np.pi/2 - angle)\r\n",
        "        else:\r\n",
        "            # this point is p3\r\n",
        "            p3_index = p_lowest\r\n",
        "            p0_index = (p3_index + 1) % 4\r\n",
        "            p1_index = (p3_index + 2) % 4\r\n",
        "            p2_index = (p3_index + 3) % 4\r\n",
        "            return poly[[p0_index, p1_index, p2_index, p3_index]], angle\r\n",
        "\r\n",
        "CHAR_VECTOR = \" 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZÉ´-~`<>'.:;^/|!?$%#@&*()[]{}_+=,\\\\\\\"\"\r\n",
        "NUM_CLASSES = len(CHAR_VECTOR) \r\n",
        "char_index={}\r\n",
        "index_char={}\r\n",
        "for i,val in enumerate(CHAR_VECTOR):\r\n",
        "  index_char[i+1]=val\r\n",
        "  char_index[val]=i+1\r\n",
        "\r\n",
        "\r\n",
        "#################################################3\r\n",
        "## CODE TO GET ORIGNAL MODEL\r\n",
        "@st.cache\r\n",
        "def load_main_models():\r\n",
        "  # This is deconv layer that we have in our text detection branch\r\n",
        "  class Deconv(tf.keras.layers.Layer):\r\n",
        "    def __init__(self,name=\"deconv\"):\r\n",
        "      super().__init__(name)\r\n",
        "      self.inp_size=0\r\n",
        "      self.conv=None\r\n",
        "      self.upsample=None\r\n",
        "      self.bn=None\r\n",
        "    def build(self,imshape):\r\n",
        "      self.inp_size=imshape\r\n",
        "      self.bn=tf.keras.layers.BatchNormalization()\r\n",
        "      self.conv=tf.keras.layers.Conv2D(filters=self.inp_size[-1]//2,kernel_size=3,padding='same',activation='relu',kernel_initializer=tf.keras.initializers.GlorotNormal(seed=12),use_bias=False)\r\n",
        "      self.upsample=tf.keras.layers.UpSampling2D(size=(2,2),interpolation='bilinear',data_format='channels_last',)\r\n",
        "    def call(self,X):\r\n",
        "      x1=self.upsample(X)\r\n",
        "      x1=self.conv(x1)\r\n",
        "      x1=self.bn(x1)\r\n",
        "      x1=tf.keras.activations.relu(x1)\r\n",
        "      return x1\r\n",
        "  resnet=tf.keras.applications.ResNet50(input_shape=(512,512,3),include_top=False,weights='imagenet')\r\n",
        "  tf.keras.backend.clear_session()\r\n",
        "  layers=resnet.layers\r\n",
        "  x1,x2,x3,x4=None,None,None,None\r\n",
        "  for i in range(len(layers)):\r\n",
        "    x=layers[i]\r\n",
        "    if x.name=='pool1_pool':\r\n",
        "      x1=x\r\n",
        "    if x.name=='conv3_block1_1_conv':\r\n",
        "      x2=x\r\n",
        "    if x.name=='conv4_block1_1_conv':\r\n",
        "      x3=x   \r\n",
        "    if x.name=='conv5_block3_2_conv':\r\n",
        "      x4=x  \r\n",
        "  #  input_1 ,conv1_relu\r\n",
        "  d=x4.output\r\n",
        "  d=Deconv('deconv1')(d)\r\n",
        "  d=tf.keras.layers.add([d,x3.output])\r\n",
        "\r\n",
        "  d=Deconv('deconv2')(d)\r\n",
        "  d=tf.keras.layers.add([d,x2.output])\r\n",
        "\r\n",
        "  d=Deconv('deconv3')(d)\r\n",
        "  d=tf.keras.layers.add([d,x1.output])\r\n",
        "  d=tf.keras.layers.BatchNormalization()(d)\r\n",
        "  d=Deconv('deconv4')(d)\r\n",
        "  d=Deconv('deconv5')(d)\r\n",
        "  score=tf.keras.layers.Conv2D(1,kernel_size=3,padding='same',activation='sigmoid')(d)\r\n",
        "\r\n",
        "  # Used this beacause sigmoid gives values in range of 0-1(as mentioned in git repository)\r\n",
        "  geo_map=tf.keras.layers.Conv2D(4,kernel_size=3,padding='same',activation='sigmoid')(d)*512\r\n",
        "  #Angles are assumed to be between [-45 to 45]\r\n",
        "  angle_map=(tf.keras.layers.Conv2D(1,kernel_size=3,padding='same',activation='sigmoid')(d)-0.5)*np.pi/2\r\n",
        "  out=tf.concat([score,geo_map,angle_map],axis=3)\r\n",
        "  detector=tf.keras.Model(resnet.input,out,name='detector')\r\n",
        "\r\n",
        "  for layers in resnet.layers:\r\n",
        "    layers.trainable=False \r\n",
        "  detector.load_weights('./drive/My Drive/detector_best.h5')\r\n",
        "  \r\n",
        "  \r\n",
        "  #Text Recognition Model\r\n",
        "  #Here I have changed the architecture a bit as mentioned in FOTS paper\r\n",
        "  inputs = tf.keras.layers.Input(name='the_input', shape=(64,128,3), dtype='float32')  \r\n",
        "\r\n",
        "  inner = tf.keras.layers.Conv2D(64, (3, 3), padding='same', name='conv1', kernel_initializer='he_normal')(inputs) \r\n",
        "  inner = tf.keras.layers.BatchNormalization()(inner)\r\n",
        "  inner = tf.keras.layers.Activation('relu')(inner)\r\n",
        "  inner = tf.keras.layers.MaxPooling2D(pool_size=(2, 1), name='max1')(inner)  \r\n",
        "\r\n",
        "  inner = tf.keras.layers.Conv2D(64, (3, 3), padding='same', name='conv2', kernel_initializer='he_normal')(inner)  \r\n",
        "  inner = tf.keras.layers.BatchNormalization()(inner)\r\n",
        "  inner = tf.keras.layers.Activation('relu')(inner)\r\n",
        "  inner = tf.keras.layers.MaxPooling2D(pool_size=(2, 1), name='max2')(inner)  \r\n",
        "\r\n",
        "  inner = tf.keras.layers.Conv2D(32, (3, 3), padding='same', name='conv3', kernel_initializer='he_normal')(inner)  \r\n",
        "  inner = tf.keras.layers.BatchNormalization()(inner)\r\n",
        "  inner = tf.keras.layers.Activation('relu')(inner)\r\n",
        "  inner = tf.keras.layers.Conv2D(32, (3, 3), padding='same', name='conv4', kernel_initializer='he_normal')(inner)  \r\n",
        "  inner = tf.keras.layers.BatchNormalization()(inner)\r\n",
        "  inner = tf.keras.layers.Activation('relu')(inner)\r\n",
        "  inner = tf.keras.layers.MaxPooling2D(pool_size=(2, 1), name='max3')(inner)  \r\n",
        "\r\n",
        "  inner = tf.keras.layers.Conv2D(32, (3, 3), padding='same', name='conv5', kernel_initializer='he_normal')(inner)  \r\n",
        "  inner = tf.keras.layers.BatchNormalization()(inner)\r\n",
        "  inner = tf.keras.layers.Activation('relu')(inner)\r\n",
        "  inner = tf.keras.layers.Conv2D(32, (3, 3), padding='same', name='conv6')(inner)   \r\n",
        "  inner = tf.keras.layers.BatchNormalization()(inner)\r\n",
        "  inner = tf.keras.layers.Activation('relu')(inner)\r\n",
        "  inner = tf.keras.layers.MaxPooling2D(pool_size=(2, 1), name='max4')(inner)  \r\n",
        "\r\n",
        "  inner = tf.keras.layers.Conv2D(64, (3, 3), padding='same', kernel_initializer='he_normal', name='con7')(inner) \r\n",
        "  inner = tf.keras.layers.BatchNormalization()(inner)\r\n",
        "  inner = tf.keras.layers.Activation('relu')(inner)\r\n",
        "  inner = tf.keras.layers.Reshape(target_shape=((64,512)), name='reshape')(inner)  \r\n",
        "  inner = tf.keras.layers.Dense(64, activation='relu', kernel_initializer='he_normal', name='dense1')(inner) \r\n",
        "\r\n",
        "  out=tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32,return_sequences=True,go_backwards=True))(inner)\r\n",
        "  out=tf.keras.layers.Bidirectional(tf.keras.layers.GRU(128,return_sequences=True,go_backwards=True))(out)\r\n",
        "  x=tf.keras.layers.Dense(100)(out)#Here we hve given 100 bcz vocab size is 99 and 1 extra is for blank symbol\r\n",
        "  x=tf.keras.activations.softmax(x)\r\n",
        "  recognizer=tf.keras.models.Model(inputs,x)\r\n",
        "  \r\n",
        "  \r\n",
        "  \r\n",
        "  recognizer.load_weights('./drive/My Drive/recognizer_best.h5')\r\n",
        "  size=os.path.getsize('./drive/My Drive/detector_best.h5')+os.path.getsize('./drive/My Drive/recognizer_best.h5')\r\n",
        "  return detector,recognizer,size\r\n",
        "\r\n",
        "## CODE TO GET DYNAMICA POST TRAINING QUNATIZATION MODEL\r\n",
        "@st.cache\r\n",
        "def load_dynamic_quantized_models():\r\n",
        "  detector_quantized=tf.lite.Interpreter(model_path=\"./drive/My Drive/quantized_detector_dynamic.tflite\")\r\n",
        "  detector_quantized.allocate_tensors()\r\n",
        "  # Get input and output tensors.\r\n",
        "  input_details_detector = detector_quantized.get_input_details()\r\n",
        "  output_details_detector = detector_quantized.get_output_details()\r\n",
        "  \r\n",
        "  #recognizer\r\n",
        "  recognizer_quantized = tf.lite.Interpreter(model_path=\"./drive/My Drive/quantized_recognized_dynamic.tflite\")\r\n",
        "  recognizer_quantized.allocate_tensors()\r\n",
        "  # Get input and output tensors.\r\n",
        "  input_details_recognizer = recognizer_quantized.get_input_details()\r\n",
        "  output_details_recognizer = recognizer_quantized.get_output_details()\r\n",
        "  size=os.path.getsize('./drive/My Drive/quantized_detector_dynamic.tflite')+os.path.getsize('./drive/My Drive/quantized_recognized_dynamic.tflite')\r\n",
        "  return (detector_quantized,input_details_detector,output_details_detector),(recognizer_quantized,input_details_recognizer,output_details_recognizer),size\r\n",
        "\r\n",
        "## CODE TO GET FLOAT16 POST TRAINING QUANTIZATION\r\n",
        "@st.cache\r\n",
        "def load_float16_quantized_models():\r\n",
        "  detector_quantized = tf.lite.Interpreter(model_path=\"./drive/My Drive/quantized_detector_float16.tflite\")\r\n",
        "  detector_quantized.allocate_tensors()\r\n",
        "  # Get input and output tensors.\r\n",
        "  input_details_detector = detector_quantized.get_input_details()\r\n",
        "  output_details_detector = detector_quantized.get_output_details()\r\n",
        "  \r\n",
        "  #Recognizer\r\n",
        "  recognizer_quantized = tf.lite.Interpreter(model_path=\"./drive/My Drive/quantized_recognized_float16.tflite\")\r\n",
        "  recognizer_quantized.allocate_tensors()\r\n",
        "  # Get input and output tensors.\r\n",
        "  input_details_recognizer = recognizer_quantized.get_input_details()\r\n",
        "  output_details_recognizer = recognizer_quantized.get_output_details()  \r\n",
        "  size=os.path.getsize('./drive/My Drive/quantized_detector_float16.tflite')+os.path.getsize('./drive/My Drive/quantized_recognized_float16.tflite')\r\n",
        "  return (detector_quantized,input_details_detector,output_details_detector),(recognizer_quantized,input_details_recognizer,output_details_recognizer),size\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "## MAIN FUNCTION\r\n",
        "def main():\r\n",
        "  st.title(\"FOTS: Scene Text Parsing\")\r\n",
        "  st.subheader(\"Deployment of FOTS Scene text Parsing\")\r\n",
        "  menu =[\"Orignal Model\",\"Dynamic Post Training Qunatized Model\",\"Float16 Post Training Qunatized Model\"]\r\n",
        "  choice = st.sidebar.selectbox('Models',menu)\r\n",
        "  uploaded_file = st.sidebar.file_uploader(\"Choose an image...\", type=[\"jpg\",\"jpeg\",\"png\"])\r\n",
        "  if uploaded_file is not None:\r\n",
        "    img = Image.open(uploaded_file)\r\n",
        "    image=np.array(img)\r\n",
        "    image=cv2.resize(image,(512,512))\r\n",
        "    \r\n",
        "    ## ORIGNAL MODEL\r\n",
        "    if choice== 'Orignal Model' and st.sidebar.button('predict'):\r\n",
        "      st.subheader('Orignal Model')\r\n",
        "      detector,recognizer,size=load_main_models()\r\n",
        "      img=image.copy()\r\n",
        "      start_time=time.time()\r\n",
        "      ii=detector.predict(np.expand_dims(img,axis=0))\r\n",
        "      score_map=ii[0][:,:,0]\r\n",
        "      geo_map=ii[0][:,:,1:]\r\n",
        "      for ind in [0,1,2,3,4]:\r\n",
        "        geo_map[:,:,ind]*=score_map\r\n",
        "      rotateParas=get_boxes(score_map,geo_map)\r\n",
        "      txt=[]\r\n",
        "      pts=[]\r\n",
        "      if len(rotateParas) > 0:\r\n",
        "        for num in range(len(rotateParas)):\r\n",
        "          text=\"\"\r\n",
        "          out=rotateParas[num][0]\r\n",
        "          crop=rotateParas[num][1]\r\n",
        "          points=np.array([[out[0],out[1]],[out[0]+out[2],out[1]],[out[0]+out[2],out[1]+out[3]],[out[0],out[1]+out[3]]])\r\n",
        "          angle=rotateParas[num][2] \r\n",
        "          img1=tf.image.crop_to_bounding_box(img,out[1],out[0],out[3],out[2])\r\n",
        "          img2=tf.keras.preprocessing.image.random_rotation(img1,angle)\r\n",
        "          img2=tf.image.crop_to_bounding_box(img2,crop[1],crop[0],crop[3],crop[2]).numpy()\r\n",
        "          img2=cv2.resize(img2,(128,64))\r\n",
        "          img2=cv2.detailEnhance(img2)\r\n",
        "          ii=recognizer.predict(np.expand_dims(img2,axis=0))\r\n",
        "          arr=tf.keras.backend.ctc_decode(ii,np.ones((1),'int8')*64,)\r\n",
        "          for val in arr[0][0].numpy()[0]:\r\n",
        "            if val==-1:\r\n",
        "              break\r\n",
        "            else:\r\n",
        "              text+=index_char[val]\r\n",
        "          txt.append(text)\r\n",
        "          pts.append(points)\r\n",
        "      for i in range(len(txt)):\r\n",
        "        cv2.polylines(img,[pts[i]],isClosed=True,color=(255,255,0),thickness=2)\r\n",
        "        cv2.putText(img,txt[i],(pts[i][0][0],pts[i][0][1]),cv2.FONT_HERSHEY_SIMPLEX,0.6, (0, 0, 255), 2)\r\n",
        "      end_time=time.time()\r\n",
        "      st.image(img)\r\n",
        "      st.subheader(\"Latency=\"+str(end_time-start_time)+\" seconds\") \r\n",
        "      st.subheader('Model Size='+str(size/2**20)+'MB')\r\n",
        "\r\n",
        "    ## DYNAMIC POST TRAINING QUANTIZATION  \r\n",
        "    elif choice=='Dynamic Post Training Qunatized Model' and st.sidebar.button('predict'):\r\n",
        "      st.subheader('Dynamic Post Training Qunatized Model')\r\n",
        "      det,rec,size=load_dynamic_quantized_models()\r\n",
        "      start_time=time.time()\r\n",
        "      img=image.copy()\r\n",
        "      img1=img.astype('float32')\r\n",
        "      det[0].set_tensor(det[1][0]['index'],np.expand_dims(img1,axis=0))\r\n",
        "      det[0].invoke()\r\n",
        "      ii=det[0].get_tensor(det[2][0]['index'])\r\n",
        "      score_map=ii[0][:,:,0]\r\n",
        "      geo_map=ii[0][:,:,1:]\r\n",
        "\r\n",
        "\r\n",
        "      for ind in [0,1,2,3,4]:\r\n",
        "        geo_map[:,:,ind]*=score_map\r\n",
        "\r\n",
        "      rotateParas=get_boxes(score_map,geo_map)\r\n",
        "      txt=[]\r\n",
        "      pts=[]\r\n",
        "      if len(rotateParas) > 0:\r\n",
        "        for num in range(len(rotateParas)):\r\n",
        "          text=\"\"\r\n",
        "          out=rotateParas[num][0]\r\n",
        "          crop=rotateParas[num][1]\r\n",
        "          points=np.array([[out[0],out[1]],[out[0]+out[2],out[1]],[out[0]+out[2],out[1]+out[3]],[out[0],out[1]+out[3]]])\r\n",
        "          angle=rotateParas[num][2] \r\n",
        "          img1=tf.image.crop_to_bounding_box(img,out[1],out[0],out[3],out[2])\r\n",
        "          img2=tf.keras.preprocessing.image.random_rotation(img1,angle)\r\n",
        "          img2=tf.image.crop_to_bounding_box(img2,crop[1],crop[0],crop[3],crop[2]).numpy()\r\n",
        "          img2=cv2.resize(img2,(128,64))\r\n",
        "          img2=cv2.detailEnhance(img2)\r\n",
        "          img2=img2.astype('float32')\r\n",
        "\r\n",
        "          rec[0].set_tensor(rec[1][0]['index'],np.expand_dims(img2,axis=0))\r\n",
        "          rec[0].invoke()\r\n",
        "          ii=rec[0].get_tensor(rec[2][0]['index'])\r\n",
        "          arr=tf.keras.backend.ctc_decode(ii,np.ones((1),'int8')*64)\r\n",
        "          for val in arr[0][0].numpy()[0]:\r\n",
        "            if val==-1:\r\n",
        "              break\r\n",
        "            else:\r\n",
        "              text+=index_char[val]\r\n",
        "          txt.append(text)\r\n",
        "          pts.append(points)\r\n",
        "      for i in range(len(txt)):\r\n",
        "        cv2.polylines(img,[pts[i]],isClosed=True,color=(255,255,0),thickness=2)\r\n",
        "        cv2.putText(img,txt[i],(pts[i][0][0],pts[i][0][1]),cv2.FONT_HERSHEY_SIMPLEX,0.6, (0, 0, 255), 2)\r\n",
        "      end_time=time.time()\r\n",
        "      st.image(img)\r\n",
        "      st.subheader(\"Latency=\"+str(end_time-start_time)+\" seconds\")\r\n",
        "      st.subheader('Dynamic Quantized Model Size='+str(size/2**20)+'MB')\r\n",
        "    \r\n",
        "    ## FLOAT16 POST TRAINING QUANTIZATION\r\n",
        "    if choice=='Float16 Post Training Quantized Model' and st.sidebar.button('predict'):\r\n",
        "      st.subheader('Float16 Qunatization')\r\n",
        "      det,rec,size=load_float16_quantized_models()\r\n",
        "      start_time=time.time()\r\n",
        "      img=image.copy()\r\n",
        "      img1=img.astype('float32')\r\n",
        "      det[0].set_tensor(det[1][0]['index'],np.expand_dims(img1,axis=0))\r\n",
        "      det[0].invoke()\r\n",
        "      ii=det[0].get_tensor(det[2][0]['index'])\r\n",
        "      score_map=ii[0][:,:,0]\r\n",
        "      geo_map=ii[0][:,:,1:]\r\n",
        "\r\n",
        "\r\n",
        "      for ind in [0,1,2,3,4]:\r\n",
        "        geo_map[:,:,ind]*=score_map\r\n",
        "\r\n",
        "      rotateParas=get_boxes(score_map,geo_map)\r\n",
        "      txt=[]\r\n",
        "      pts=[]\r\n",
        "      if len(rotateParas) > 0:\r\n",
        "        for num in range(len(rotateParas)):\r\n",
        "          text=\"\"\r\n",
        "          out=rotateParas[num][0]\r\n",
        "          crop=rotateParas[num][1]\r\n",
        "          points=np.array([[out[0],out[1]],[out[0]+out[2],out[1]],[out[0]+out[2],out[1]+out[3]],[out[0],out[1]+out[3]]])\r\n",
        "          angle=rotateParas[num][2] \r\n",
        "          img1=tf.image.crop_to_bounding_box(img,out[1],out[0],out[3],out[2])\r\n",
        "          img2=tf.keras.preprocessing.image.random_rotation(img1,angle)\r\n",
        "          img2=tf.image.crop_to_bounding_box(img2,crop[1],crop[0],crop[3],crop[2]).numpy()\r\n",
        "          img2=cv2.resize(img2,(128,64))\r\n",
        "          img2=cv2.detailEnhance(img2)\r\n",
        "          img2=img2.astype('float32')\r\n",
        "\r\n",
        "          rec[0].set_tensor(rec[1][0]['index'],np.expand_dims(img2,axis=0))\r\n",
        "          rec[0].invoke()\r\n",
        "          ii=rec[0].get_tensor(rec[2][0]['index'])\r\n",
        "          arr=tf.keras.backend.ctc_decode(ii,np.ones((1),'int8')*64)\r\n",
        "          for val in arr[0][0].numpy()[0]:\r\n",
        "            if val==-1:\r\n",
        "              break\r\n",
        "            else:\r\n",
        "              text+=index_char[val]\r\n",
        "          txt.append(text)\r\n",
        "          pts.append(points)\r\n",
        "      for i in range(len(txt)):\r\n",
        "        cv2.polylines(img,[pts[i]],isClosed=True,color=(255,255,0),thickness=2)\r\n",
        "        cv2.putText(img,txt[i],(pts[i][0][0],pts[i][0][1]),cv2.FONT_HERSHEY_SIMPLEX,0.6, (0, 0, 255), 2)\r\n",
        "      end_time=time.time()\r\n",
        "      st.image(img)\r\n",
        "      st.subheader(\"Latency=\"+str(end_time-start_time)+\" seconds\")\r\n",
        "\r\n",
        "\r\n",
        "      st.subheader('Float16 Quantized Model Size='+str(size/2**20)+'MB')\r\n",
        "if __name__ == '__main__':\r\n",
        "  main()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zM8lpQvv97fM",
        "outputId": "1fedd39a-273c-4073-89b5-6eaa0c3254a2"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "app.py\tdrive  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DatHBKFE-UKh",
        "outputId": "971ae690-15ec-4a44-9681-0005e370ebef"
      },
      "source": [
        "from pyngrok import ngrok\r\n",
        "public_url = ngrok.connect(port='80')\r\n",
        "print (public_url)\r\n",
        "!streamlit run --server.port 80 app.py >/dev/null"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NgrokTunnel: \"http://1a17232e22e6.ngrok.io\" -> \"http://localhost:80\"\n",
            "2021-01-14 09:48:48.322183: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-01-14 09:49:31.067971: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-01-14 09:49:31.070034: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
            "2021-01-14 09:49:31.080917: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2021-01-14 09:49:31.080981: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (5e5d669556bd): /proc/driver/nvidia/version does not exist\n",
            "2021-01-14 09:49:31.081743: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-01-14 09:49:35.090917: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
            "2021-01-14 09:49:35.091406: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2299995000 Hz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swKfRVtk-Y89"
      },
      "source": [
        "ngrok.kill()"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YT9GdhV244St"
      },
      "source": [
        "tunnels = ngrok.get_tunnels()"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnuYRbqKIUqw"
      },
      "source": [
        "ngrok.disconnect('http://e829503992da.ngrok.io/')"
      ],
      "execution_count": 13,
      "outputs": []
    }
  ]
}